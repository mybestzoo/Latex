\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{graphicx}
\usepackage[hscale=0.8,vscale=0.8]{geometry}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{conseq}{Consequence}

\title{\LARGE \bf RESEARCH STATEMENT}
\author{Tigran Bagramyan (t.bagramyan@me.com)}
\date{}

\begin{document}
\maketitle


	My research lies in the field of functional analysis concerning different problems of approximation, optimization and inverse problems. In recent studies I've developed the framework of optimal recovery theory and its applications to the problems of classical analysis, integral geometry and statistical estimation. This approach allows to find the features of objects from the noisy measurements by the methods, which are optimal on certain classes of objects. It finds its explicit applications in signal processing, image reconstruction, recovery of solutions of differential equations, statistical estimation, financial mathematics.
	
	\subsection*{Carried out research}
	%\hspace{\parindent}
	
	In many applied and theoretical problems one needs to recover a function (functional or operator) from the information, which can be incomplete or given with an error. Such problems are investigated in optimal recovery theory - a modern branch of approximation theory, which has it roots in works of A.N. Kolmogorov (notion of Kolmogorov widths) and then first appear in PhD thesis of S.A. Smolyak (\cite{SM}) and developed lately by C.A. Michelli, T.J. Rivlin (\cite{MR,MR1}), Traub, Vozhnyakovski (\cite{TW}), K.Yu. Osipenko, M.I. Stessin, G.G. Magaril- Il'yaev (\cite{O,OS,MO3}). In general the problem is to find the best approximation of a value of linear operator $U:X\rightarrow Z$ on a given set $X$ from values of another linear operator $I:X\rightarrow Y$ (called information) given with an error in some metric. The problem can be demonstrated on the following diagram:
	
	\includegraphics[scale=0.8]{diagram.jpg}
	
Here $X$ is a linear space	and $Y$ and $Z$ are normed linear spaces. $W$ is a subset of $X$, $U$ is a linear operator from $X$ into $Z$ (the {\it feature} operator) and $I$ is a linear operator from $X$ into $Y$ (the {\it information} operator). An arbitrary mapping $m:Y\rightarrow Z$ is called a {\it method of recovery}. Suppose, that instead of the element $Ix$ we are given its approximation $y\in Y$ with an error $\delta$, i.e. $\|Ix-y\|\le\delta$. Each method of recovery $m$ produces an {\it error}

	$$
	e(\delta,m) =  \sup\{\|Ux-my\|: x\in W, \|Ix-y\|\le\delta}
	$$
	
and 
	$$
	E(\delta) = \inf_{m} e(\delta,m)
	$$
is called {\it the error of the optimal recovery}.
The method of recovery $m$ is optimal if the error of the optimal recovery $E(\delta)$ is achieved by the error $e(\delta,m)$ of $m$, i.e. $e(\delta,m)=E(\delta)$.
Particular cases usually use information in a form of a linear functional or an operator that maps function to the set of it's values in a number of nodes, it's Fourier coefficients or the function itself. This covers a large set of numerical methods problems, such as quadrature formulas, optimal interpolation and approximation, recovery of the solutions of differential equations, signal reconstruction, optimal state estimation and others. The solutions for some of this problems and general results for certain cases when $I$ is a functional, or $X$, $Y$, $Z$ are Hilbert spaces and $\delta=0$ can be found in \cite{MR}. The main result can be conceptually formulated as the statement that among all the optimal methods there exist a linear optimal method. The new approach based on the general results from the extremal problems theory and Lagrange principle was introduced in works of K.Yu. Osipenko, G.G. Magaril- Il'yaev (\cite{MO3,O,MO,OS}) and allowed to find solution in a series of complex problems with linear operators in non Hilbert spaces and $\delta>0$. Though it was shown that in this generality the linear optimal method may not exist, the obtained results are important examples of the types of problems, when this principle remains true. In my research I developed this approach and implemented it to the problem of reconstruction of functions from their Radon transform (in a broad sense) - operator, that maps function on a manifold to the set of it's integrals over some family of submanifolds (in classical case --- integration along all the hyperplanes in space). Operators of this type are typically considered in integral geometry (\cite{H,Sa}) and computerized tomography, which investigates numerical methods of function recovery from it's linear or plane integrals. When the Radon transform of a function is known there exist inversion formulas, that perform a unique recovery. My goal was to deal with uncertain information, when the only thing that is known a priori is the error of measurements. In optimal recovery theory some similar problems were considered previously by B.F. Logan, L.A. Shepp (their work \cite{LS} considered function in a unit circle on a plane and it's Radon transform measured in a finite number of directions) and by A.J. Degraw (who considered in his PhD thesis and \cite{D} the problem of optimal recovery of harmonic function in a unit ball on a plane from it's Radon transform and radial integration operator, which measured with an error). My first results published in \cite{B} considered the Hardy space of harmonic functions in a unit ball of $\mathbb R^d$, where the information was the radial integration operator measured with an error in $L_2$ and $L_\infty$ norms. I proceeded with a more complex case of the Radon transform acting on the same class of functions and inaccurately measured in $L_2$ norm. The solution to this problem was published in \cite{B1} and also included a new inequality for the Radon transform, which was obtained as a consequence of the considered optimal recovery problem solution. The latest and most general result in this area (submitted to {\it Inverse Problems}) considers the generalization of the Radon transform -- the so-called k-plane transform, which maps functions to their integrals along k-dimensional planes in $\mathbb R^d$ (\cite{K}). The class of functions here is also broader (and better suits for purposes of applications in tomography), than the class of harmonic functions from Hardy space. It consists of functions in $L_2(\mathbb R^d)$ which have a bounded $\alpha-$th degree of the Laplace operator. This work includes new inversion methods of optimal recovery of functions from their noisy k-plane transform as well as corresponding inequalities for the k-plane transform and the Laplace operator. Particular cases include the results for the classical Radon and X-ray transforms.     
	
Among other applications of optimal recovery theory, that attracted my interest was the classical analysis problem of inequalities for derivatives. In recent works \cite{OS,MO3} authors obtained some inequalities for derivatives and showed, that the problem of finding the exact constants in such inequalities can be formulated and efficiently solved as the corresponding optimal recovery problem. I considered an optimal recovery problem for the k-th derivative of the function on an interval from the information on the function itself, given in the mean square metric. As a consequence of the solution I obtained one Kolmogorov type inequality for derivatives on an interval. An important corollary demonstrated that the constant in this inequality can be reduced by considering particular subsets of the function
class. In particular cases I presented explicit expressions for this subsets and corresponding constants in my publication \cite{B2}.

\subsection*{Future research}

In the previous discussion of optimal recovery from inaccurate information the point of view was that the inaccurate data $y\in Y$ had the form $y=Ix+e$, where $Ix$ is the exact data corresponding to some $x\in W$ and $e$ is in the $\delta-$ball about the origin in $Y$. From this point of view every $e$ in this $\delta-$ball is equally likely. If we consider the possibility that $e$ is a random variable we come to the definition of the optimal recovery problem with {\it stochastic information}. Now the error of the method $m$ is the worst expected value of the discrepancy,i.e.
	$$
	\sup_{x\in W} E (\|Ux-my\|),
	$$
and the objective is to minimize this error over all $m:Y\rightarrow Z$.
This problem is mentioned in \cite{MR}, but the known results use a significant departure from our previous approach as they assume that only linear methods $m$ are considered. However, in paper \cite{Do} D.L. Donoho demonstrated, that the problem of {\it minimax statistical estimation} (which assumes $e$ to be a zero-mean Gaussian noise) and the problem of optimal recovery have a number of underlying similarities. For a particular case we can approach the problem in two different ways: one time assuming the noise is random Gaussian, and the other time assuming the noise is chosen by antagonist, subject to a quadratic constraint. In some cases both ways of stating the problem have been solved, and what happens is that while the two solutions are different in detail, they belong to the same family - that is the same family of splines, of kernel estimators or of regularized least square estimates - only the tuning constants are chosen differently. The similarity discovered by D.L. Donoho has a fundamental character, which gives a rise to opportunity to apply the recent developments in optimal recovery theory (based on the Lagrange principle from general extremal problems) to the problems of optimal recovery with stochastic information. This results will be applicable to the particular problems of {\it minimax statistical estimation, nonparametric regression, density estimation} and others.

Thus we briefly discussed the research area motivated by assumption that the information in optimal recovery problem has a stochastic nature. However this is not the only possible way to introduce stochastic principles in optimal recovery. So far we have always assumed that any element $x\in W$ had an equally likely chance to be chosen when estimating $Ux$. This led us to the worst case criterion, which required to take the supremum of $\|Ux-my\|$ over $x\in W$. By allowing some randomness in the choice of $x$ we define a new problem, which is called the {\it stochastic optimal recovery}. This is accomplished by using a probability measure on $X$. This problem is formulated in \cite{MR} in a restricted to exact information case ($\delta=0$). Let $X=W$ be a Hilbert space and $\mu$ a Borel measure defined on $X$. Suppose $I:X\rightarrow\mathbb R^d$ is a continuous linear operator and $U$ is a continuous linear operator from $X$ into $Z$ which is a Hilbert space. We wish to estimate $Ux$, $x\in X$, by a linear algorithm $m$ and find such an algorithm with least mean square error

	$$
	e(m) = \int_{X}\|Ux-m(Ix)\|^2\mu(dx).
	$$
Here the significant constraints are the assumptions that the information is exact, that information operator has a finite rank and that we restrict possible methods to those which are linear. Any of these constraints represent an open problem to investigate the assumptions under which they can be avoided.	
Described problems represent a significant interest from theoretical side, but also are important for applications as they provide a possibility to establish new robust numerical methods. We mentioned statistical estimation as one of such applications, but there are definitely much more others, among which: theoretical development of optimal recovery in general case of operator recovery from information on the values of another operator (not yet fully investigated); optimal methods of function recovery from its Radon transform measured in a  finite number of directions; recovery of function from its Radon transform in case of Elliptic (Riemann) and Hyperbolic (Lobachevsky) geometries; development of classical inequalities for derivatives based on the approach of optimal recovery.  Among the applied problems, that attract my attention is the problem of derivatives pricing that arise in financial mathematics, and can be presented as the problem of optimal recovery of the derivative price as a functional of the underlying asset price. There are various ways to define an a priori information on the asset price, i.e. to define a class $W$. It can be an analytical constraint (defined by a differential operator, like those classes that we use for the Radon transform) or an assumption that the return of underlying asset belongs to the family of solutions of a certain partial differential equation, also the stochastic nature of the price can be introduced by considering stochastic PDEs. The choice of the information operator will also lead to different formulations of the problem. The measurements can be presented by a stock price in a certain moment of time, volatility of a stock, or a combination of those and others characteristics.    
The proposed research combines abstract mathematical theory and real applications that bring interest of mathematicians and specialists from industry together. I consider it to be a very prospective and challenging research area to which I'm putting my efforts.

\bibliography{bibliography}

\end{document}
